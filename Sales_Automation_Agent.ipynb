{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4t987XNLZh1Y/SGK5dldf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitaa4/Sales-Assistant-Agent/blob/main/Sales_Automation_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhsLxJhJN8vD"
      },
      "outputs": [],
      "source": [
        "# installing required libraries\n",
        "!pip install -q streamlit langchain langchain-google-genai tavily-python beautifulsoup4 requests lxml langchain-community -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import streamlit.components.v1 as components\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# --- 1. UTILITY & TOOL DEFINITIONS ---\n",
        "\n",
        "SOCIAL_PLATFORMS = {\n",
        "    'linkedin.com': 'LinkedIn', 'twitter.com': 'Twitter', 'facebook.com': 'Facebook',\n",
        "    'instagram.com': 'Instagram', 'youtube.com': 'YouTube'\n",
        "}\n",
        "\n",
        "def scrape_contact_info(url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Completely rebuilt scraper that is much more accurate and less prone to false positives.\n",
        "    \"\"\"\n",
        "    if not url.startswith('http'):\n",
        "        url = 'https://' + url\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "\n",
        "    found_info = {\n",
        "        \"email\": set(), \"phone\": set(), \"social_links\": {}, \"contact_page\": set(),\n",
        "        \"meta_title\": \"Not found\", \"meta_description\": \"Not found\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=100)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "        base_url = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n",
        "\n",
        "        # --- Scrape meta tags for better context ---\n",
        "        if soup.title and soup.title.string:\n",
        "            found_info[\"meta_title\"] = soup.title.string.strip()\n",
        "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "        if meta_desc and meta_desc.get('content'):\n",
        "            found_info[\"meta_description\"] = meta_desc.get('content').strip()\n",
        "\n",
        "        # Find all links on the page\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "\n",
        "        for a in all_links:\n",
        "            href = a['href']\n",
        "            # Find direct email addresses\n",
        "            if href.startswith('mailto:'):\n",
        "                found_info[\"email\"].add(href.replace('mailto:', ''))\n",
        "\n",
        "            # Stricter check for social media and contact pages\n",
        "            full_url = urljoin(base_url, href)\n",
        "            parsed_full_url = urlparse(full_url)\n",
        "            domain = parsed_full_url.netloc.lower().replace('www.', '')\n",
        "            path = parsed_full_url.path.lower()\n",
        "\n",
        "            # Social media: Must match the domain\n",
        "            if domain in SOCIAL_PLATFORMS:\n",
        "                platform_name = SOCIAL_PLATFORMS[domain]\n",
        "                if platform_name not in found_info[\"social_links\"]: # Add only the first unique link\n",
        "                    found_info[\"social_links\"][platform_name] = full_url\n",
        "\n",
        "            # Contact page: Must have 'contact' or 'help' in the URL path\n",
        "            if '/contact' in path or '/help' in path:\n",
        "                found_info[\"contact_page\"].add(full_url)\n",
        "\n",
        "        # Fallback regex search for emails and phones\n",
        "        page_text = soup.get_text()\n",
        "        found_info[\"email\"].update(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', page_text))\n",
        "        found_info[\"phone\"].update(re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', page_text))\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        st.warning(f\"Could not scrape {url}: {e}\")\n",
        "\n",
        "    final_output = {\n",
        "        \"Email Address\": list(found_info[\"email\"])[0] if found_info[\"email\"] else \"Not found\",\n",
        "        \"Phone Number\": list(found_info[\"phone\"])[0] if found_info[\"phone\"] else \"Not found\",\n",
        "        \"Social Media\": found_info[\"social_links\"] if found_info[\"social_links\"] else \"Not found\",\n",
        "        \"Contact Form Page\": list(found_info[\"contact_page\"])[0] if found_info[\"contact_page\"] else \"Not found\",\n",
        "        \"Website Title\": found_info[\"meta_title\"],\n",
        "        \"Website Description\": found_info[\"meta_description\"]\n",
        "    }\n",
        "    return final_output\n",
        "\n",
        "\n",
        "PLATFORM_BLACKLIST = [\n",
        "    'wikipedia', 'crunchbase', 'bloomberg', 'reuters', 'github', 'zoominfo',\n",
        "    'forbes', 'techcrunch', 'medium', 'news', 'jobs', 'careers', 'owler'\n",
        "]\n",
        "\n",
        "def clean_company_name(name: str) -> str:\n",
        "    name = name.lower()\n",
        "    suffixes = ['inc', 'llc', 'ltd', 'corp', 'corporation', 'gmbh']\n",
        "    for suffix in suffixes:\n",
        "        name = name.replace(f'.{suffix}', '').replace(f',{suffix}', '').replace(suffix, '')\n",
        "    name = re.sub(r'[\\s.,-]', '', name)\n",
        "    return name\n",
        "\n",
        "def find_best_url(company_name: str, search_results: list) -> str | None:\n",
        "    \"\"\"\n",
        "    Finds the most likely official URL from search results using a scoring system.\n",
        "    It heavily prioritizes URLs where the domain contains the company name.\n",
        "    \"\"\"\n",
        "    clean_name = clean_company_name(company_name)\n",
        "    best_score = -1\n",
        "    best_url = None\n",
        "\n",
        "    # This blacklist is for non-corporate sites like news, directories, etc.\n",
        "    # Social media is handled by the main scraper, so it's not needed here.\n",
        "    NON_CORPORATE_BLACKLIST = [\n",
        "        'wikipedia', 'crunchbase', 'bloomberg', 'reuters', 'github', 'zoominfo',\n",
        "        'forbes', 'techcrunch', 'medium', 'news', 'jobs', 'careers', 'owler', 'apollo.io'\n",
        "    ]\n",
        "\n",
        "    for result in search_results:\n",
        "        url = result.get('url')\n",
        "        title = result.get('title', '')\n",
        "        if not url:\n",
        "            continue\n",
        "\n",
        "        current_score = 0\n",
        "\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            domain = parsed_url.netloc.lower().replace('www.', '')\n",
        "\n",
        "            # --- HEURISTIC SCORING ---\n",
        "\n",
        "            # 1. Penalize heavily if it's a known non-corporate platform.\n",
        "            if any(platform in domain for platform in NON_CORPORATE_BLACKLIST):\n",
        "                current_score -= 100\n",
        "\n",
        "            # 2. **HIGHEST REWARD**: If the clean company name is a part of the domain.\n",
        "            # This is the logic that worked well before and is now prioritized.\n",
        "            # Example: 'wipro' in 'wipro.com' -> TRUE\n",
        "            if clean_name in domain:\n",
        "                current_score += 100\n",
        "\n",
        "            # 3. Small bonus if the company name is in the page title.\n",
        "            # This helps break ties.\n",
        "            if clean_name in title.lower():\n",
        "                current_score += 10\n",
        "\n",
        "            # Update the best URL if the current one has a higher score\n",
        "            if current_score > best_score:\n",
        "                best_score = current_score\n",
        "                best_url = url\n",
        "\n",
        "        except Exception:\n",
        "            # If any URL is malformed, just skip it.\n",
        "            continue\n",
        "\n",
        "    # Return the URL that scored the highest.\n",
        "    return best_url\n",
        "\n",
        "PITCH_TEMPLATES = {\n",
        "    \"Formal\": \"a professional and respectful tone\", \"Casual\": \"a friendly, approachable tone\",\n",
        "    \"Direct & Punchy\": \"a direct, to-the-point tone with a sense of urgency\", \"Follow-up\": \"a gentle, concise follow-up tone\"\n",
        "}\n",
        "st.set_page_config(page_title=\"Sales Assistant Agent\", layout=\"wide\")\n",
        "st.title(\"AI Sales Assistant Agent\")\n",
        "with st.sidebar:\n",
        "    st.header(\"Configuration\")\n",
        "    gemini_api_key = st.text_input(\"Gemini API Key:\", type=\"password\", key=\"gemini_api_key\")\n",
        "    tavily_api_key = st.text_input(\"Tavily API Key:\", type=\"password\", key=\"tavily_api_key\")\n",
        "    st.markdown(\"---\"); st.subheader(\"Your Information\")\n",
        "    user_name = st.text_input(\"Your Name:\", placeholder=\"e.g., Jane Doe\", key=\"user_name\")\n",
        "    user_title = st.text_input(\"Your Company/Title:\", placeholder=\"e.g., Founder, Innovate Inc.\", key=\"user_title\")\n",
        "    st.markdown(\"---\"); st.subheader(\"Pitch Settings\")\n",
        "    pitch_tone = st.selectbox(\"Select Pitch Tone:\", options=list(PITCH_TEMPLATES.keys()), key=\"pitch_tone\")\n",
        "\n",
        "company_name = st.text_input(\"Enter Company Name to research:\", key=\"company_name\")\n",
        "if st.button(\"Generate Email\"):\n",
        "    if not all([gemini_api_key, tavily_api_key, company_name]):\n",
        "        st.warning(\"Please enter your API keys and a company name.\")\n",
        "    else:\n",
        "        with st.spinner(\"Executing sales agent workflow...\"):\n",
        "            try:\n",
        "                llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=gemini_api_key, temperature=0.7)  #please please if required toggle the\n",
        "                search_tool = TavilySearchResults(tavily_api_key=tavily_api_key, max_results=5)\n",
        "\n",
        "                st.write(\"Step 1: Finding official website...\")\n",
        "                search_results = search_tool.invoke(f\"official website homepage for {company_name}\")\n",
        "                website_url = find_best_url(company_name, search_results)\n",
        "                if not website_url: st.error(f\"Could not find a reliable website for '{company_name}'.\"); st.stop()\n",
        "\n",
        "                website_url = f\"{urlparse(website_url).scheme}://{urlparse(website_url).netloc}\"\n",
        "                st.write(f\"Found and cleaned URL: {website_url}\")\n",
        "\n",
        "                st.write(\"Step 2: Performing hyper-scrape for context and contacts...\")\n",
        "                contact_info = scrape_contact_info(website_url)\n",
        "\n",
        "                # --- THE DEFINITIVE, HYPER-PERSONALIZED PROMPT ---\n",
        "                prompt_template = ChatPromptTemplate.from_template(\n",
        "                    \"\"\"\n",
        "                    **TASK:** Generate a complete, ready-to-send cold email pitching an invented B2B AI automation service.\n",
        "\n",
        "                    **CORE INSTRUCTIONS:**\n",
        "                    1.  **HYPER-PERSONALIZE THE OPENING:** Your first sentence MUST be a specific, compelling observation based on the \"Website Title\" and \"Website Description\" provided below. Do NOT use generic openings like \"I was looking at your website.\" Instead, connect their stated mission to a problem your invented service can solve.\n",
        "                    2.  **INVENT A RELEVANT AUTOMATION SERVICE:** Create a plausible AI service that solves a problem relevant to the target company. Do NOT use generic placeholders.\n",
        "                    3.  **WRITE IN PROSE ONLY (NO BULLET POINTS):** The entire email body must be in natural paragraphs. Do NOT use lists. Weave the benefits smoothly into the text.\n",
        "                    4.  **SIGN-OFF CORRECTLY:** Use the provided sender name and title.\n",
        "                    5.  **FORMAT:** Start with a compelling subject line: 'Subject: Your Subject Here'.\n",
        "\n",
        "                    **CONTEXT FOR PERSONALIZATION:**\n",
        "                    - **Website Title:** {website_title}\n",
        "                    - **Website Description:** {website_description}\n",
        "\n",
        "                    **EMAIL DETAILS:**\n",
        "                    - **Target Company:** {company_name}\n",
        "                    - **Sender Name:** {user_name}\n",
        "                    - **Sender Title/Company:** {user_title}\n",
        "                    - **Desired Tone:** {pitch_style}\n",
        "\n",
        "                    Execute the task now.\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "                st.write(\"Step 3: Drafting hyper-personalized email...\")\n",
        "                email_generation_chain = prompt_template | llm | StrOutputParser()\n",
        "                email_text = email_generation_chain.invoke({\n",
        "                    \"company_name\": company_name, \"website_title\": contact_info[\"Website Title\"],\n",
        "                    \"website_description\": contact_info[\"Website Description\"], \"user_name\": user_name,\n",
        "                    \"user_title\": user_title, \"pitch_style\": PITCH_TEMPLATES[pitch_tone]\n",
        "                })\n",
        "\n",
        "                st.subheader(\"Agent Finished: Email Draft Ready\")\n",
        "                st.text_area(\"Generated Email\", email_text, height=400, key=\"email_text_area\")\n",
        "                escaped_text = json.dumps(email_text)\n",
        "                button_html = f\"\"\"<style>.copy-btn-container{{...}}</style><div class=\"copy-btn-container\"><button ...>...</button></div>\"\"\"\n",
        "                components.html(button_html, height=40)\n",
        "\n",
        "                with st.expander(\"View Agent's Findings\", expanded=True):\n",
        "                    st.json(contact_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Fdb7Ye3UON4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "uZme1LbbTCeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "C27aLuWlTFwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}